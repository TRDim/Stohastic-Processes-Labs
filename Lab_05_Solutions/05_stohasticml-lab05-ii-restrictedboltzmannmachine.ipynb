{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><b>Restricted Boltzmann Machine</b></h1>\n<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε τον τρόπο λειτουργίας μιας <i>RBM (<a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machine</a>)</i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.Το συγκεκριμένο πρόγραμμα αξιοποιεί το <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">dataset του <i>MNIST</i></a>, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας.</p>\n<p align=\"justify\">Μία αρκετά σημαντική εφαρμογή της <i>RBM</i> είναι η εξαγωγή χαρακτηριστικών (feature representation) από ένα dataset με σκοπό την αναπαράσταση της εισόδου (ορατοί νευρώνες) με ένα διάνυσμα μικρότερης διάστασης (κρυφοί νευρώνες). Στη συγκεκριμένη άσκηση θα συγκρίνετε την ακρίβεια ενός ταξινομητή ψηφίων με τη χρήση του αλγορίθμου <i>Logistic Regression</i>, όταν εκείνος δέχεται ως είσοδο το dataset (i) αφου υποστεί επεξεργασία από το <i>RBM</i> και (ii) χωρίς να έχει υποστεί επεξεργασία από το <i>RBM</i>.</p>\n<p align=\"justify\">Το δοθέν <i>dataset</i>, στα πλαίσια της άσκησης έχει\nδιογκωθεί με τεχνητό τρόπο (γραμμική μετατόπιση ενός εικονοστοιχείου (pixel) σε\nκάθε κατεύθυνση) ώστε να έχουμε ένα <i>dataset</i> 5 φορές μεγαλύτερο. Με βάση τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n<ul>\n<li>1. Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας <i>RBM</i>. Τι διαφορές έχει σε σχέση με μία <i> Μηχανή Boltzmann</i>;</li>\n<li>2. Να αναφέρετε τις βασικότερες εφαρμογές μιας RBM.</li>\n<li>3. Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση RBM σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>\n<li>4. Επιχειρήστε να αλλάξετε τον αριθμό των κρυφών νευρώνων (components) από 100 σε 200. Τι\nπαρατηρείτε ως προς τα αποτελέσματα και τον χρόνο που έτρεξε το\nπρόγραμμα μέχρι να γίνει <i>fit</i>΄;</li>\n<li>5. Προσπαθήστε να τρέξετε το πρόγραμμα τρέξτε το πρόγραμμα εκ νέου με το <i>κανονικό dataset</i>. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων; Πού μπορεί να οφείλονται τυχόν αποκλίσεις σε σχέση με πριν;</li>\n</ul>","metadata":{"id":"mxJ9DwJ4qB3c"}},{"cell_type":"code","source":"from __future__ import print_function\n\nprint(__doc__)\n\n# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.ndimage import convolve\nfrom sklearn import linear_model, datasets, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport time \n\n\n# #############################################################################\n# Setting up\n\ndef nudge_dataset(X, Y):\n    \"\"\"\n    This produces a dataset 5 times bigger than the original one,\n    by moving the 8x8 images in X around by 1px to left, right, down, up\n    \"\"\"\n    direction_vectors = [\n        [[0, 1, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [1, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 1],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 1, 0]]]\n\n    def shift(x, w):\n        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()\n\n    X = np.concatenate([X] +\n                       [np.apply_along_axis(shift, 1, X, vector)\n                        for vector in direction_vectors])\n    Y = np.concatenate([Y for _ in range(5)], axis=0)\n    return X, Y\n\n\n# Load Data\ndigits = datasets.load_digits()\nX = np.asarray(digits.data, 'float32')\nX, Y = nudge_dataset(X, digits.target)\nX = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X, Y, test_size=0.2, random_state=0)\n\n# Models we will use\nlogistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,\n                                           multi_class='multinomial')\nrbm = BernoulliRBM(random_state=0, verbose=True)\n\nrbm_features_classifier = Pipeline(\n    steps=[('rbm', rbm), ('logistic', logistic)])\n\n# #############################################################################\n# Training\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 20\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000\n\n\n# Training RBM-Logistic Pipeline\nstart = time.time()\nrbm_features_classifier.fit(X_train, Y_train)\ntotal_time = time.time() - start \n\nprint('********************************************')\nprint(' RBM Training Time with LogReg: ', total_time)\nprint('********************************************')\n\n# Training the Logistic regression classifier directly on the pixel\nraw_pixel_classifier = clone(logistic)\nraw_pixel_classifier.C = 100.\nraw_pixel_classifier.fit(X_train, Y_train)\n\n# #############################################################################\n# Evaluation\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\"Logistic regression using RBM features:\\n%s\\n\" % (\n    metrics.classification_report(Y_test, Y_pred)))\n\nY_pred = raw_pixel_classifier.predict(X_test)\nprint(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n    metrics.classification_report(Y_test, Y_pred)))\n\n# #############################################################################\n# Plotting\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(rbm.components_):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle('100 components extracted by RBM', fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()","metadata":{"id":"pFbLBrfZrM5w","outputId":"398cc680-fc10-4de7-f438-eb5c0ce333bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aπαντήσεις:\n\n1. **RBM**: Στοχαστικοί νευρώνες 2 επιπέδων (ορατό & κρυφό), με συμμετρικές συνάψεις και καταστάσεις [0,1]. Οι καταστάσεις των ορατών νευρώνων κωδικοποιούν χαρακτηριστικά $v_i$ τα οποία είναι παρατηρήσιμα (δειγματα εισόδου-εξόδου) ενώ του κρυφού επιπέδου $h_j$ κωδικοποιούν κρυφές ιδιότητες. Η συνάρτηση ενεργοποίησης είναι η Sigmoid και τα δεδομένα την διαπερνούν μετά απο άθροιση των συναπτικών βαρών και τον bias και των 2 καταστασεων νευρώνων (ορατών & κρυφών).  Η κύρια διαφορά με το **ΒΜ** έχει να κάνει με την συνδεσιμότητα των νευρώνων των ιδίων επιπέδων όπου στο **RΒΜ**  είναι ασύνδετοι επιταχύνοντας έτσι την παραγωγή δείγματος με στατιστική ομοιότητα με τα δεδομένα μάθησης. \n\n2. Βασικότερες Εφαρμογές RBM:\n* dimensionality reduction\n* classification\n* collaborative filtering \n* feature learning \n* Incorporate them in Deep Learning Architecture\n\n3. Τα αποτελέσματα με χρήση της Logistic Regression & RBM είναι σαφώς καλύτερα από ότι με την απλή Logistic Regression. H χρήση RBM στην περίπτωση αυτή  λειτουργεί ως feature extraxtion τα οποία οδηγούν σε σωστότερες αποφάσεις-προβλέψεις. Επίσης να σημειωθεί ότι η χρήση της RBM αντιμετωπίζει καλύτερα το συγκεκριμένο datasetοπου έχει εφαρμοστέι data augmentation με την χρήση shifts. Στην περίπτωση της απλής Logistic Regression αυτός ο εμπλουτισμός σε πολλές περιπτώσεις λειτουργεί ως θόρυβος στα δεδομένα καθώς είναι πιθανόν να τροποποιεί και την εικόνα τους και συνεπώς να οδηγεί σε λανθασμένες προβλέψεις.\n\n4. Η αύξηση των νευρώνων (100->200) δεν συντελεί σε βελτίωση των προβλεψεων παρα μόνον σε αύξηση του χρόνου εκπαίδευσης. Ένα συμπερασμα θα μπορούσε να είναι ότι οι υπάρχοντες (νευρωνες) αρκούν για την εκτίμηση του συγκεκριμένου ντατασετ και μιας και ασύνδετοι, η αύξηση του αριθμού τους προσφέρει μονάχα υπολογιστική πολυπλοκότητα.\n\n5. Όπως επισημάναμε και στην απάντηση 3 παρατηρούμε ότι το Data Augmentation δεν οδήγησε σε σημαντικη βελτίωση του μοντέλου με RBM και αντιθέτως στη περίπτωση με το απλό Logistic Regression οδήγησε σε αρκετά χειρότερα αποτελέσματα. Χωρίς data augmentation το Logistic Regression πέτυχε πολυ καλύτερα αποτελέσματα.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}