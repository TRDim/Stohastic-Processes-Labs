{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Q-Learning</h1>\n\n<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείίγματος θα εξετάσετε μίία υλοποίηση με q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n<ul>\n<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n<li>Να τηρούνται οι κανόόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n</ul>\n\n<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n\n<h4>Επιβράβευση</h4>\n<ul>\n<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n</ul>\n\n<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n\n<h4>Πλήθος Καταστάσεων</h4>\n<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n\n<h4>Πλήθος Ενεργειών</h4>\n<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n<ul>\n<li>0=Νότια</li>\n<li>1=Βόρεια</li>\n<li>2=Ανατολικά</li>\n<li>3=Δυτικά</li>\n<li>4=Επιβίβαση</li>\n<li>5=Αποβίβαση</li>\n</ul>\n\n\n\n","metadata":{"id":"gHFOjD_VY0ld"}},{"cell_type":"markdown","source":"<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n\n<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>","metadata":{"id":"HOQpQ0DygDzt"}},{"cell_type":"markdown","source":"O Αλγόριθμος Q-learning ειναι μια τεχνική της ενισχυστικής μάθησης, που λειτουργεί εκτιμώνατας τη συνάρτηση Q(S,A) για να μάθει μια καλή ή βέλτιστη πολιτική για τον κόσμο του πράκτορα χωρίνα να ξερει το μοντέλο του κόσμου (model-free). Μια βασική προυπόθεση για την εφαρμογή του αλγορίθμου είναι να μπντελοποιείται ο κόσμπς ώς Markov Process (η επόμνη κατάσταση ανεξάρτητη απο τό ιστορικό των κινήσεων και εξαρτάται μονάχα απο την τορινή κτάσταση).Ξεκινώντας απο την τωρινή κατάσταση ο αλγόριθμος βρίσκει την βέλτιστη πολιτική μεγιστοποιώντας την ανταμοιβή σε όλα συνολικά τα βήματα που επιλέγει. \nΠροβλήματα στα οποία ζητείται να βρεθεί η βέλτιστη διαδρομή, προβήματα γράφων, ή ακόμα και σε προβλήματα classification είναι αυτά στα οποία ο συγκεκριμένος αλγόριθμος βρίσκει εφαρμογή. \nΗ διαφορά με τους αλγορίθμους Policy & Value Iteration είναι οτι εκ των προτερων οι πιθανότητες μετάβασης αλλα και τα οφέλη είναι άγνωστα- δεν υπάρχει συγκεκριμένο μοντέλο που ακολουθείται. Μετά απο την πραγματοποίηση μιας κίνησης του agent γνωστοποιείται η ανταμοιβή για την συγκεκριμένη μετάβαση. Οι μελλοντικές μεταβάσεις γνωστοποιούντια μονάχα όταν ο agend πρέπει να πάρει μια απόφαση απο την συγκεκριμένη θέση. Για στοχαστικές μεταβάσεις οι ο αλγόριθμος εκπαιδεύεται πανω στις πιθανότητες μετάβασης από την επαναληψημότητα μιας κατάστασης. ","metadata":{}},{"cell_type":"markdown","source":"<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>","metadata":{"id":"pc11bCDZgfSx"}},{"cell_type":"code","source":"!pip install cmake 'gym[atari]' scipy","metadata":{"id":"y_95lIAzWYpp","execution":{"iopub.status.busy":"2021-05-25T16:41:50.670493Z","iopub.execute_input":"2021-05-25T16:41:50.671111Z","iopub.status.idle":"2021-05-25T16:42:01.982053Z","shell.execute_reply.started":"2021-05-25T16:41:50.670989Z","shell.execute_reply":"2021-05-25T16:42:01.980931Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting cmake\n  Downloading cmake-3.20.2-py2.py3-none-manylinux1_x86_64.whl (19.4 MB)\n\u001b[K     |████████████████████████████████| 19.4 MB 4.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: gym[atari] in /opt/conda/lib/python3.7/site-packages (0.18.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (1.5.4)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.19.5)\nRequirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (7.2.0)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.0)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.6.0)\nRequirement already satisfied: opencv-python>=3. in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (4.5.1.48)\nCollecting atari-py~=0.2.0\n  Downloading atari_py-0.2.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[K     |████████████████████████████████| 2.8 MB 27.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\nInstalling collected packages: atari-py, cmake\nSuccessfully installed atari-py-0.2.9 cmake-3.20.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import gym\n\nenv = gym.make(\"Taxi-v3\").env\n\nenv.render()","metadata":{"id":"NDHLb5PGWdwr","execution":{"iopub.status.busy":"2021-05-25T16:42:01.985254Z","iopub.execute_input":"2021-05-25T16:42:01.985560Z","iopub.status.idle":"2021-05-25T16:42:03.110446Z","shell.execute_reply.started":"2021-05-25T16:42:01.985530Z","shell.execute_reply":"2021-05-25T16:42:03.109341Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| |\u001b[43m \u001b[0m: | : |\n|Y| : |\u001b[34;1mB\u001b[0m: |\n+---------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"env.reset() # reset environment to a new, random state\nenv.render()\n\nprint(\"Action Space {}\".format(env.action_space))\nprint(\"State Space {}\".format(env.observation_space))\n","metadata":{"id":"396kTtOrW-cO","execution":{"iopub.status.busy":"2021-05-25T16:42:03.112202Z","iopub.execute_input":"2021-05-25T16:42:03.112641Z","iopub.status.idle":"2021-05-25T16:42:03.119741Z","shell.execute_reply.started":"2021-05-25T16:42:03.112594Z","shell.execute_reply":"2021-05-25T16:42:03.118397Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"+---------+\n|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n|\u001b[43m \u001b[0m: | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\nAction Space Discrete(6)\nState Space Discrete(500)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>","metadata":{"id":"-UC6-XuIhF5_"}},{"cell_type":"code","source":"state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\nprint(\"State:\", state)\n\nenv.s = state\nenv.render()","metadata":{"id":"nPSOw5CdXFx1","execution":{"iopub.status.busy":"2021-05-25T16:42:45.703358Z","iopub.execute_input":"2021-05-25T16:42:45.703723Z","iopub.status.idle":"2021-05-25T16:42:45.709472Z","shell.execute_reply.started":"2021-05-25T16:42:45.703690Z","shell.execute_reply":"2021-05-25T16:42:45.708476Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"State: 328\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| |\u001b[43m \u001b[0m: | : |\n|\u001b[34;1mY\u001b[0m| : |B: |\n+---------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>","metadata":{"id":"NsAkQJhchVFy"}},{"cell_type":"code","source":"env.P[328]","metadata":{"id":"j7oDbznIXOJo","execution":{"iopub.status.busy":"2021-05-25T16:42:49.851714Z","iopub.execute_input":"2021-05-25T16:42:49.852094Z","iopub.status.idle":"2021-05-25T16:42:49.861815Z","shell.execute_reply.started":"2021-05-25T16:42:49.852061Z","shell.execute_reply":"2021-05-25T16:42:49.860581Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{0: [(1.0, 428, -1, False)],\n 1: [(1.0, 228, -1, False)],\n 2: [(1.0, 348, -1, False)],\n 3: [(1.0, 328, -1, False)],\n 4: [(1.0, 328, -10, False)],\n 5: [(1.0, 328, -10, False)]}"},"metadata":{}}]},{"cell_type":"markdown","source":"<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n\n<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>","metadata":{"id":"CtROeD1ph6kk"}},{"cell_type":"markdown","source":"Δεν οδηγούμαστε σε ικανοποιητικά αποτελέσματα καθώς η επιλογή κατάστασης δεν βασίζεται σε penalties ή rewards παρα είναι τυχάια.\nΑν χρησιμοποιούσαμε Q-Learning ο agent θα μάθαινε μεσω της επαναληψιμότητας τα rewards μα και τα penalties ειδικότερα τα οποία και θα απέφευγε. ","metadata":{}},{"cell_type":"code","source":"env.s = 328  # set environment to illustration's state\n\nepochs = 0\npenalties, reward = 0, 0\n\nframes = [] # for animation\n\ndone = False\n\nwhile not done:\n    action = env.action_space.sample()\n    state, reward, done, info = env.step(action)\n\n    if reward == -10:\n        penalties += 1\n    \n    # Put each rendered frame into dict for animation\n    frames.append({\n        'frame': env.render(mode='ansi'),\n        'state': state,\n        'action': action,\n        'reward': reward\n        }\n    )\n\n    epochs += 1\n    \n    \nprint(\"Timesteps taken: {}\".format(epochs))\nprint(\"Penalties incurred: {}\".format(penalties))","metadata":{"id":"vKHhcDxVXUFz","execution":{"iopub.status.busy":"2021-05-25T16:44:31.896662Z","iopub.execute_input":"2021-05-25T16:44:31.897077Z","iopub.status.idle":"2021-05-25T16:44:32.043079Z","shell.execute_reply.started":"2021-05-25T16:44:31.897041Z","shell.execute_reply":"2021-05-25T16:44:32.042240Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Timesteps taken: 2147\nPenalties incurred: 724\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"ZssyR6_cj9O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n\n<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>","metadata":{"id":"aj3s09rsizVm"}},{"cell_type":"markdown","source":"**alpha:** αναφέρεται στο ποσοστό εκμάθησης. Για στοχαστικές συναρτήσεις (ανταμοιβής / μετάβασης) η τιμή του θα πρέπει να είναι φθίνουσα με την αυξηση του αριθμου των επαναλήψεων.Το παραπάνω έχει άμεση συσχέτιση με την προσέγγιση του αναμενόμενου αποτελέσματος (Μεταβαση * Αντμοιβή) για τυχαόία συμπεριφορά τους. Η τιμή 1 αντιστοιχεί σε πολύ γρήγορη ανανέωση των q-values ενώ η τιμή 0 se καθόλου ενημέρωση ( αδυναμία μάθησης)\n\n**gamma:** ανταποκρίνεται στην αξια της ανταμοιβής. Μπορεί να είναι στατική ή δυναμική. Για την τιμή 1 έιναι σημαντική η επιβράβευση όχι μόνο για την άμεση κατάσταση αλλα και για μεταγενέστερες καταστάσεις (ελευθερια επιλογήσ της τωρινής κατάστασης). Τιμή 0 η επιβράβευση είναι σημαντική μονάχα για τις πιο αμεσες ανταμοιβές.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nq_table = np.zeros([env.observation_space.n, env.action_space.n])","metadata":{"id":"JJk3NTfcXrrA","execution":{"iopub.status.busy":"2021-05-25T16:44:39.948817Z","iopub.execute_input":"2021-05-25T16:44:39.949340Z","iopub.status.idle":"2021-05-25T16:44:39.953625Z","shell.execute_reply.started":"2021-05-25T16:44:39.949308Z","shell.execute_reply":"2021-05-25T16:44:39.952584Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\n\"\"\"Training the agent\"\"\"\n\nimport random\nfrom IPython.display import clear_output\n\n# Hyperparameters\nalpha = 0.1\ngamma = 0.6\nepsilon = 0.1\n\n# For plotting metrics\nall_epochs = []\nall_penalties = []\n\nfor i in range(1, 100001):\n    state = env.reset()\n\n    epochs, penalties, reward, = 0, 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = env.action_space.sample() # Explore action space\n        else:\n            action = np.argmax(q_table[state]) # Exploit learned values\n\n        next_state, reward, done, info = env.step(action) \n        \n        old_value = q_table[state, action]\n        next_max = np.max(q_table[next_state])\n        \n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table[state, action] = new_value\n\n        if reward == -10:\n            penalties += 1\n\n        state = next_state\n        epochs += 1\n        \n    if i % 100 == 0:\n        clear_output(wait=True)\n        print(f\"Episode: {i}\")\n\nprint(\"Training finished.\\n\")","metadata":{"id":"Oy2Yg8DTXtHW","execution":{"iopub.status.busy":"2021-05-25T16:44:44.926318Z","iopub.execute_input":"2021-05-25T16:44:44.926702Z","iopub.status.idle":"2021-05-25T16:45:58.640337Z","shell.execute_reply.started":"2021-05-25T16:44:44.926669Z","shell.execute_reply":"2021-05-25T16:45:58.639458Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Episode: 100000\nTraining finished.\n\nCPU times: user 1min 14s, sys: 22.2 s, total: 1min 37s\nWall time: 1min 13s\n","output_type":"stream"}]},{"cell_type":"code","source":"q_table[328]","metadata":{"id":"dxt4fmvGYBOm","execution":{"iopub.status.busy":"2021-05-25T16:46:18.846396Z","iopub.execute_input":"2021-05-25T16:46:18.846753Z","iopub.status.idle":"2021-05-25T16:46:18.853888Z","shell.execute_reply.started":"2021-05-25T16:46:18.846723Z","shell.execute_reply":"2021-05-25T16:46:18.852968Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([ -2.41329978,  -2.27325184,  -2.41590663,  -2.35906783,\n       -10.68215401, -10.09079863])"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n\ntotal_epochs, total_penalties,total_rewards = 0, 0, 0\nepisodes = 100\n\nfor _ in range(episodes):\n    state = env.reset()\n    epochs, penalties, reward, rewards = 0, 0, 0, 0\n    \n    done = False\n    \n    while not done:\n        action = np.argmax(q_table[state])\n        state, reward, done, info = env.step(action)\n\n        if reward == -10:\n            penalties += 1\n        else:\n            rewards +=1\n\n        epochs += 1\n\n    total_penalties += penalties\n    total_epochs += epochs\n    total_rewards +=rewards\n    \nprint(f\"Results after {episodes} episodes:\")\nprint(f\"Average timesteps per episode: {total_epochs / episodes}\")\nprint(f\"Average penalties per episode: {total_penalties / episodes}\")\nprint(f\"Average rewaards per episode: {total_rewards / episodes}\")","metadata":{"id":"6W9JE9yOYGgP","execution":{"iopub.status.busy":"2021-05-25T16:50:02.146973Z","iopub.execute_input":"2021-05-25T16:50:02.147332Z","iopub.status.idle":"2021-05-25T16:50:02.184832Z","shell.execute_reply.started":"2021-05-25T16:50:02.147301Z","shell.execute_reply":"2021-05-25T16:50:02.183673Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Results after 100 episodes:\nAverage timesteps per episode: 13.34\nAverage penalties per episode: 0.0\nAverage rewaards per episode: 13.34\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Evaluate agent's performance without Q-learning\"\"\"\nenv.reset() # reset environment to a new, random state\n\nenv.s = 328  # set environment to illustration's state\n\nepisodes = 100\n\nframes = [] # for animation\ntotal_epochs = 0\ntotal_penalties = 0\ntotal_rewards = 0\ndone = False\nfor _ in range(episodes):\n  state = env.reset()\n  epochs, penalties, reward, rewards = 0, 0, 0, 0\n  while not done:\n      action = env.action_space.sample()\n      state, reward, done, info = env.step(action)\n\n      if reward == -10:\n          penalties += 1\n      else:\n          rewards += 1\n      \n      # Put each rendered frame into dict for animation\n      frames.append({\n          'frame': env.render(mode='ansi'),\n          'state': state,\n          'action': action,\n          'reward': reward\n          }\n      )\n\n      epochs += 1\n  total_epochs += epochs\n  total_penalties += penalties\n  total_rewards += rewards\n    \nprint(f\"Results after {episodes} episodes:\")\nprint(f\"Average timesteps per episode: {total_epochs / episodes}\")\nprint(f\"Average penalties per episode: {total_penalties / episodes}\")\nprint(f\"Average rewards per episode: {total_rewards / episodes}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:50:42.776176Z","iopub.execute_input":"2021-05-25T16:50:42.776527Z","iopub.status.idle":"2021-05-25T16:50:42.852393Z","shell.execute_reply.started":"2021-05-25T16:50:42.776496Z","shell.execute_reply":"2021-05-25T16:50:42.851187Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Results after 100 episodes:\nAverage timesteps per episode: 9.68\nAverage penalties per episode: 2.9\nAverage rewards per episode: 6.78\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n<ul>\n<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n</ul>\n<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n</b>","metadata":{"id":"5R7gW1nLj-qE"}},{"cell_type":"markdown","source":"Με την χρηση Q-Learing καταλήγουμε σε καλύτερα αποτελέσματα όπως μπορούμε να δούμε στην εκτέλεση των παραπάνω.\n\n#### Me Q-Learning\nResults after 100 episodes:\n\n- Average timesteps per episode: 13.34\n\n- Average penalties per episode: 0.0\n\n- Average rewaards per episode: 13.34\n\n#### Xωρίς Q-Learnign\nResults after 100 episodes:\n\n- Average timesteps per episode: 9.68\n\n- Average penalties per episode: 2.9\n\n- Average rewards per episode: 6.78\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}