{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><b>Markov Decision Processes</h1></b>\n<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n","metadata":{"id":"XLZdEbAy2jfr"}},{"cell_type":"markdown","source":"<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>","metadata":{"id":"6VsUV229__zO"}},{"cell_type":"code","source":"import numpy as np\nimport gym\nfrom gym import wrappers","metadata":{"id":"OM8ivgOJAg_H","execution":{"iopub.status.busy":"2021-05-25T14:58:34.71488Z","iopub.execute_input":"2021-05-25T14:58:34.715295Z","iopub.status.idle":"2021-05-25T14:58:35.014285Z","shell.execute_reply.started":"2021-05-25T14:58:34.715214Z","shell.execute_reply":"2021-05-25T14:58:35.01271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:","metadata":{"id":"_puV3ugeAnbU"}},{"cell_type":"code","source":"env_name = 'FrozenLake8x8-v0'\nenv = gym.make(env_name)","metadata":{"id":"Ep-MvIUCAxT8","execution":{"iopub.status.busy":"2021-05-25T15:05:57.599722Z","iopub.execute_input":"2021-05-25T15:05:57.600095Z","iopub.status.idle":"2021-05-25T15:05:58.505173Z","shell.execute_reply.started":"2021-05-25T15:05:57.600063Z","shell.execute_reply":"2021-05-25T15:05:58.504386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent","metadata":{"id":"uBKBXJDUBRUh"}},{"cell_type":"code","source":"env.reset()\nenv.render()","metadata":{"id":"p6lqbG9zBgdi","execution":{"iopub.status.busy":"2021-05-25T15:06:10.306071Z","iopub.execute_input":"2021-05-25T15:06:10.306437Z","iopub.status.idle":"2021-05-25T15:06:10.312664Z","shell.execute_reply.started":"2021-05-25T15:06:10.306399Z","shell.execute_reply":"2021-05-25T15:06:10.311458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα.","metadata":{"id":"FX2res4JBlYb"}},{"cell_type":"code","source":"next_action = env.action_space.sample()\nenv.step(next_action)\nenv.render()","metadata":{"id":"Gq7q944YBx0Q","execution":{"iopub.status.busy":"2021-05-25T15:16:36.806222Z","iopub.execute_input":"2021-05-25T15:16:36.80684Z","iopub.status.idle":"2021-05-25T15:16:36.81407Z","shell.execute_reply.started":"2021-05-25T15:16:36.806795Z","shell.execute_reply":"2021-05-25T15:16:36.812334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;","metadata":{"id":"mV4A7lsLB54y"}},{"cell_type":"markdown","source":"**Οι κινήσεις είναι στοχαστικές (περιέχουν κάποια πιθανότητα για το άν θα συμβούν ή όχι) συνεπώς ο agent δεν πραγματοποιεί πάντα τις επιθυμητές κινήσεις.**","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Ερωτήσεις</b></h2>","metadata":{"id":"PAL4we3gDV_J"}},{"cell_type":"markdown","source":"<ul>\n<li>1. Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n<li>2. Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n<li>3. Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n<li>4. Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\nπρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\nIteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n</ul>","metadata":{"id":"zQKm4VAUChi1"}},{"cell_type":"markdown","source":"### Aπαντήσεις \n1. Το πρόβλημα της παγωμένης λίμνης είναι ένα πρόβλημα με στοχαστικές κινήσεις ενός παίκτη τοποθετημένος επάνω σε ένα πλέγμα με την δυνατότητα κίνησης -πάνω -κάτω -δεξιά -αριστερά. Ωστόσο λόγω του ότι η λίμνη είναι παγωμένη και γλιστράει υπάρχει πιθανότητα παραμονής στην ίδια θέση (στοχαστικότητα κινήσεων). Ο σκοπός είναι ξεκινώντας από μια αρχική θέση να καταφέρει να βρεθεί στην θέση όπου ένα φρίσμπι έχει προσγηωθεί. Πρόκειται για ένα πρόβλημα reinforcement learning όπου ο agent προσπαθεί να βρεί το βέλτιστο μονοπάτι προς τον στόχο (target). To grid περιγράφεται με τις παρακάτω κινήσεις και η επιβράβευση πραγματοποιείται είτε όταν καταφέρουμε να φτάσουμε τον στόχο και επιβραβεύεται με 1, είτε άν καταλήξουμε στο νερό όπου βαθμολογείται με 0. \n\n> SFFF       (S: starting point, safe)\n\n> FHFH       (F: frozen surface, safe)\n\n> FFFH       (H: hole, fall to your doom)\n\n> HFFG       (G: goal, where the frisbee is located)","metadata":{}},{"cell_type":"markdown","source":"2. Σύμφωνα με την ιδιότητα Μarkov οι επόμενες κινήσεις, η πιθανότητα δηλαδή πραγματοποίησής τους είναι ανεξάρτητη από το ιστορικό των κινήσεων που έχουν πραγματοποιηθεί εως την παρούσα κατάσταση και εξαρτάται μονάχα απο την δεδομένη κατάσταση (memoryless). Στο συγκεκριμένο πρόβλημα, η πιθανότητα μετακίνησεις σε μια επόμενη κατάσταση σε 2 διαφορετικές περιπτώσεις πραγματοποιώντας την ίδια ενέργεια είναι ίδια και ανεξάρτητη απο το τι έχει προηγηθεί στα προγενέστερα σημεία. ","metadata":{}},{"cell_type":"markdown","source":"3. **Policy Iteration:** Για μια τυχαία πολιτική υπολογίζονται τα Values-Q που αντιστοιχούν στην συγκεκριμένη επιλογή, σε επόμενη κατάσταση επιλέγεται άλλη πολιτική και υπολογίζονται τα νεα Q-Values. Αφού καταγραφούν όλα τα trajectories των πολιτικών που ακολουθήθηκαν ο αλγόριμος Q-learning οδηγεί το σύστημα στη μάιηση βέλτιστης πολιτικής κατα προσέγγιση. Λειτουργεί για βελτιστοποίηση της συγκεκριμένης πολιτικής (Policy Improvement)\n\n**Value Iteration:** Mια Value Funcion επιλέγεται τυχαία και ακολουθώντας μια επαναληπτική διαδικασια μια πιο βελτιωμένη value function επιλέγεται, η διαδικασία αυτη επαναλαμβάνεται εως ότου η βέλτιστη εξ'αυτών προκύψει. Λειτουργέι προσπαθώντας να βρεί την βέλτιση συνολικά πολιτική (Οptimal policy. \n\nΜε την κατάλληλη χωρητικότητα αναμένεται και οι 2 αλγόριθμοι να συγκλινουν στην βέλτιστη πολιτική. Ωστόσο, ο policy iteration αναμένεται να βρεί την καλύτερη Value Funtion ενώ ο value iteration θα βρέι την βέλτιστη απο αυτές που συνολικά έχουν επιλεγέι (δεν πραγματοποιεί βελτιστοποίηση στην αρχική value function που έχει επιλεγέι αλλά αλλάζει διαρκώς). Επόσξς απο τα παραπάνω καταλαβαίνουμε ότι ο τρόπος λειτουργίας της Policy iteration είναι απλόυστερος άρα και αναμένεται να συγκλινει γρηγορότερα.","metadata":{}},{"cell_type":"markdown","source":"4. Όπως ήταν αναμενόμενο ο policy iteration(τετραγωνική σύγκλιση) συγκλίνει σημαντικά γρηγορότερα απο τον value iteration (γραμμική συγκλιση) . ","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Policy Iteration</b></h2>","metadata":{"id":"S6mci5P4HJ_1"}},{"cell_type":"code","source":"\"\"\"\nSolving FrozenLake8x8 environment using Policy iteration.\nAuthor : Moustafa Alzantot (malzantot@ucla.edu)\n\"\"\"\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\nimport time \n\n\ndef run_episode(env, policy, gamma = 1.0, render = False):\n    \"\"\" Runs an episode and return the total reward \"\"\"\n    obs = env.reset()\n    total_reward = 0\n    step_idx = 0\n    while True:\n        if render:\n            env.render()\n        obs, reward, done , _ = env.step(int(policy[obs]))\n        total_reward += (gamma ** step_idx * reward)\n        step_idx += 1\n        if done:\n            break\n    return total_reward\n\n\ndef evaluate_policy(env, policy, gamma = 1.0, n = 100):\n    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n    return np.mean(scores)\n\ndef extract_policy(v, gamma = 1.0):\n    \"\"\" Extract the policy given a value-function \"\"\"\n    policy = np.zeros(env.nS)\n    for s in range(env.nS):\n        q_sa = np.zeros(env.nA)\n        for a in range(env.nA):\n            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n        policy[s] = np.argmax(q_sa)\n    return policy\n\ndef compute_policy_v(env, policy, gamma=1.0):\n    \"\"\" Iteratively evaluate the value-function under policy.\n    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n    and solve them to find the value function.\n    \"\"\"\n    v = np.zeros(env.nS)\n    eps = 1e-10\n    while True:\n        prev_v = np.copy(v)\n        for s in range(env.nS):\n            policy_a = policy[s]\n            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n        if (np.sum((np.fabs(prev_v - v))) <= eps):\n            # value converged\n            break\n    return v\n\ndef policy_iteration(env, gamma = 1.0):\n    \"\"\" Policy-Iteration algorithm \"\"\"\n    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n    max_iterations = 200000\n    gamma = 1.0\n    for i in range(max_iterations):\n        old_policy_v = compute_policy_v(env, policy, gamma)\n        new_policy = extract_policy(old_policy_v, gamma)\n        if (np.all(policy == new_policy)):\n            print ('Policy-Iteration converged at step %d.' %(i+1))\n            break\n        policy = new_policy\n    return policy\n\n\nif __name__ == '__main__':\n    env_name  = 'FrozenLake8x8-v0'\n    env = gym.make(env_name)\n    env = env.unwrapped\n    start = time.time()\n    optimal_policy = policy_iteration(env, gamma = 1.0)\n    end = time.time()-start\n    print('Time estimation for policy iteration: {}s'.format(end))\n    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n    print('Average scores = ', np.mean(scores))","metadata":{"id":"_43MjfJ9G8v7","execution":{"iopub.status.busy":"2021-05-25T15:45:48.574273Z","iopub.execute_input":"2021-05-25T15:45:48.574736Z","iopub.status.idle":"2021-05-25T15:45:53.964903Z","shell.execute_reply.started":"2021-05-25T15:45:48.574695Z","shell.execute_reply":"2021-05-25T15:45:53.963909Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Policy-Iteration converged at step 11.\nTime estimation for policy iteration: 5.200650930404663s\nAverage scores =  1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2><b>Value Iteration</b></h2>","metadata":{"id":"gcikBq6BHRQM"}},{"cell_type":"code","source":"\"\"\"\nSolving FrozenLake8x8 environment using Value-Itertion.\nAuthor : Moustafa Alzantot (malzantot@ucla.edu)\n\"\"\"\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\n\ndef run_episode(env, policy, gamma = 1.0, render = False):\n    \"\"\" Evaluates policy by using it to run an episode and finding its\n    total reward.\n    args:\n    env: gym environment.\n    policy: the policy to be used.\n    gamma: discount factor.\n    render: boolean to turn rendering on/off.\n    returns:\n    total reward: real value of the total reward recieved by agent under policy.\n    \"\"\"\n    obs = env.reset()\n    total_reward = 0\n    step_idx = 0\n    while True:\n        if render:\n            env.render()\n        obs, reward, done , _ = env.step(int(policy[obs]))\n        total_reward += (gamma ** step_idx * reward)\n        step_idx += 1\n        if done:\n            break\n    return total_reward\n\n\ndef evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n    \"\"\" Evaluates a policy by running it n times.\n    returns:\n    average total reward\n    \"\"\"\n    scores = [\n            run_episode(env, policy, gamma = gamma, render = False)\n            for _ in range(n)]\n    return np.mean(scores)\n\ndef extract_policy(v, gamma = 1.0):\n    \"\"\" Extract the policy given a value-function \"\"\"\n    policy = np.zeros(env.nS)\n    for s in range(env.nS):\n        q_sa = np.zeros(env.action_space.n)\n        for a in range(env.action_space.n):\n            for next_sr in env.P[s][a]:\n                # next_sr is a tuple of (probability, next state, reward, done)\n                p, s_, r, _ = next_sr\n                q_sa[a] += (p * (r + gamma * v[s_]))\n        policy[s] = np.argmax(q_sa)\n    return policy\n\n\ndef value_iteration(env, gamma = 1.0):\n    \"\"\" Value-iteration algorithm \"\"\"\n    v = np.zeros(env.nS)  # initialize value-function\n    max_iterations = 100000\n    eps = 1e-20\n    for i in range(max_iterations):\n        prev_v = np.copy(v)\n        for s in range(env.nS):\n            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n            v[s] = max(q_sa)\n        if (np.sum(np.fabs(prev_v - v)) <= eps):\n            print ('Value-iteration converged at iteration# %d.' %(i+1))\n            break\n    return v\n\n\nif __name__ == '__main__':\n    env_name  = 'FrozenLake8x8-v0'\n    gamma = 1.0\n    env = gym.make(env_name)\n    env = env.unwrapped\n    \n    start = time.time()\n    optimal_v = value_iteration(env, gamma);\n    end = time.time()-start\n    print('Time estimation for value iteration: {}s'.format(end))\n    \n    policy = extract_policy(optimal_v, gamma)\n    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n    print('Policy average score = ', policy_score)","metadata":{"id":"gHvcnTDcHGmH","execution":{"iopub.status.busy":"2021-05-25T15:46:07.769621Z","iopub.execute_input":"2021-05-25T15:46:07.769979Z","iopub.status.idle":"2021-05-25T15:46:12.269324Z","shell.execute_reply.started":"2021-05-25T15:46:07.769948Z","shell.execute_reply":"2021-05-25T15:46:12.268047Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Value-iteration converged at iteration# 2357.\nTime estimation for value iteration: 2.5902700424194336s\nPolicy average score =  1.0\n","output_type":"stream"}]}]}