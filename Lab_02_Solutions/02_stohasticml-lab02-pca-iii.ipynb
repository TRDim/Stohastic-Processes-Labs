{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><b><i>Principal Component Analysis</i> (<i>PCA</i>) </b></h1>\n\n<p>Στην άσκηση αυτή θα μελετήσετε τον αλγόριθμο <b><i>ανάλυσης σε κύριες συνιστώσες</i></b> (<b><i>Principal Component Analysis</i></b>, <b><i>PCA</i></b>), υλοποιημένο σύμφωνα με τη <b><i>μέθοδο συνδιακύμανσης</i></b> (<b><i>covariance method</i></b>). Για να κατανοήσετε τη χρησιμότητα της μεθόδου θα εκπαιδεύσετε και θα αξιολογήσετε την ακρίβεια ενός μοντέλου <b><i>logistic regression</i></b> για ένα dataset πριν και μετά την εφαρμογή του αλγορίθμου <b><i>PCA</i></b>. Περισσότερες πληροφορίες για τη μέθοδο αυτή μπορείτε να αναζητήσετε <a href=\"https://ourarchive.otago.ac.nz/handle/10523/7534\">εδώ</a>.</p>\n\n<p>Η άσκηση περιλαμβάνει <b><i>δύο</i></b> προγράμματα <i>Python</i>: (a) το πρώτο δέχεται ένα dataset σε μορφή .<i>csv</i>, εφαρμόζει τον αλγόριθμο <b><i>PCA</i></b> και δημιουργεί το αρχείο <b><i>foo.csv</i></b> με το μετασχηματισμένο dataset, όπως προκύπτει από τις κύριες συνιστώσες που επέλεξε ο χρήστης να διατηρήσει, (b) το δεύτερο δέχεται ένα αρχείο σε μορφή .<i>csv</i>, διαχωρίζει το dataset σε <i>training</i> και <i>test set</i>, εκπαιδεύει ένα μοντέλο <b><i>logistic regression</i></b>, χρησιμοποιώντας το <i>training set</i> και υπολογίζει τον αριθμό των σφαλμάτων του μοντέλου πάνω στο <i>test set</i>.</p>\n\n<p>Το dataset που θα χρησιμοποιήσετε παρέχεται σε δύο μορφές: (a) <b><i><a href=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\">demo3a.csv</a></i></b> και (b) <b><i><a href=\"https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab2/demo3b.csv\">demo3b.csv</a></i></b>, το οποίο δεν περιλαμβάνει την πρώτη στήλη του <b><i><a href=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\">demo3a.csv</a></i></b>, δηλαδή τα <i>labels</i> που αντιστοιχούν σε κάθε είσοδο. Τα datasets αυτά αποτελούν απλοποιημένη μορφή του dataset που μπορεί να βρεθεί <a href=\"https://archive.ics.uci.edu/ml/datasets/wine\">εδώ</a>.</p>\n\n<h3><b><i>Ανάλυση σε Κύριες Συνιστώσες</i></b></h3>\n<p>Αρχικά, θα φορτώσετε τις βιβλιοθήκες που απαιτούνται για το πρόγραμμα που θα αναλύσει το dataset <b><i><a href=\"https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab2/demo3b.csv\">demo3b.csv</a></i></b> στις κύριες συνιστώσες του.</p>\n\n\n","metadata":{"id":"s_xjbOW-TkC3"}},{"cell_type":"code","source":"#!pip install numpy\n#https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\nfrom numpy import genfromtxt\nfrom numpy import mean\nfrom numpy import cov\nfrom numpy.linalg import eig\nimport numpy as np","metadata":{"id":"KDcaQZ2ofeum","outputId":"68e1a244-2720-43f2-d7f0-524f5ef882ef","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<p>Στη συνέχεια, θα φορτώσετε το dataset <b><i></i></b></p>","metadata":{"id":"pMilxe9Uf-WT"}},{"cell_type":"code","source":"data = genfromtxt('https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3b.csv', delimiter=',')\nprint(data.shape) # (130,13)","metadata":{"id":"k92Gv9dwftZp","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(130, 13)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Έπειτα, θα υπολογίσετε το μέσο όρο κάθε στήλης (feature) του dataset και θα κανονικοποιήσετε κάθε feature με αυτόν</p>","metadata":{"id":"ydl-AyxahG2b"}},{"cell_type":"code","source":"M = mean(data.T, axis=1) # (13)\ndata_normal = data - M","metadata":{"id":"QU4TQXw2g4xF","trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<p>Στη συνέχεια, θα υπολογίσετε το <b><i>πίνακα συνδιακύμανσης</i></b> (<b><i>covariance matrix</i></b>) για το dataset</p>","metadata":{"id":"BYWD8ozQh-h8"}},{"cell_type":"code","source":"covariance = cov(data_normal.T)\nprint(\"The covariance matrix of the normalized data is the following: \")\nprint(covariance)","metadata":{"id":"N3B6Pw6diHxe","outputId":"f77a72d3-6b85-454f-e023-66ed6ca7825f","trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The covariance matrix of the normalized data is the following: \n[[ 7.89911157e-01  1.64350328e-02  5.03965534e-02 -1.39557561e+00\n   4.51870543e+00  2.35123047e-01  3.56524806e-01 -2.92133512e-02\n   9.15320632e-02  1.07205560e+00  3.92238044e-03  1.22678110e-01\n   2.36909466e+02]\n [ 1.64350328e-02  7.74344335e-01  3.18039952e-02  4.24165474e-01\n  -2.21434109e-01  1.44160704e-02  3.75581395e-02  5.34385808e-03\n   6.47891175e-02 -1.54797734e-01 -6.06599761e-02  6.96007454e-02\n  -3.33629219e+01]\n [ 5.03965534e-02  3.18039952e-02  8.83052117e-02  3.73006798e-01\n   1.40136434e+00  4.12452594e-02  8.26573643e-02  5.81539654e-03\n   1.24709839e-02  1.22397007e-01  2.06346094e-03  3.02871079e-02\n   3.18556160e+01]\n [-1.39557561e+00  4.24165474e-01  3.73006798e-01  1.15631181e+01\n  -6.46193798e+00 -4.24464222e-01 -4.52046512e-01  1.24282469e-01\n  -1.77910614e-01 -2.39565069e+00 -2.05387955e-02 -1.02069171e-03\n  -5.11872284e+02]\n [ 4.51870543e+00 -2.21434109e-01  1.40136434e+00 -6.46193798e+00\n   2.36586822e+02  2.55096899e+00  2.89720930e+00 -3.57263566e-01\n   2.30737209e+00  8.63970543e+00  1.85996899e-01  9.56255814e-01\n   2.31515659e+03]\n [ 2.35123047e-01  1.44160704e-02  4.12452594e-02 -4.24464222e-01\n   2.55096899e+00  2.97453936e-01  3.40611628e-01 -2.64587657e-02\n   1.30694305e-01  5.23527370e-01 -7.63816339e-04  1.28195856e-01\n   9.73171139e+01]\n [ 3.56524806e-01  3.75581395e-02  8.26573643e-02 -4.52046512e-01\n   2.89720930e+00  3.40611628e-01  5.44297674e-01 -2.88767442e-02\n   2.16111628e-01  8.48084496e-01 -8.24186047e-04  1.88436434e-01\n   1.41942171e+02]\n [-2.92133512e-02  5.34385808e-03  5.81539654e-03  1.24282469e-01\n  -3.57263566e-01 -2.64587657e-02 -2.88767442e-02  1.18999463e-02\n  -1.98501670e-02 -4.97212642e-02  9.44753727e-04 -2.42901670e-02\n  -1.26918819e+01]\n [ 9.15320632e-02  6.47891175e-02  1.24709839e-02 -1.77910614e-01\n   2.30737209e+00  1.30694305e-01  2.16111628e-01 -1.98501670e-02\n   2.91137680e-01  2.39266834e-01 -9.77493143e-04  8.78196959e-02\n   5.22928014e+01]\n [ 1.07205560e+00 -1.54797734e-01  1.22397007e-01 -2.39565069e+00\n   8.63970543e+00  5.23527370e-01  8.48084496e-01 -4.97212642e-02\n   2.39266834e-01  2.64316778e+00  2.68330829e-03  1.60809159e-01\n   4.44340177e+02]\n [ 3.92238044e-03 -6.06599761e-02  2.06346094e-03 -2.05387955e-02\n   1.85996899e-01 -7.63816339e-04 -8.24186047e-04  9.44753727e-04\n  -9.77493143e-04  2.68330829e-03  2.84562519e-02 -8.13836136e-03\n   6.93777746e+00]\n [ 1.22678110e-01  6.96007454e-02  3.02871079e-02 -1.02069171e-03\n   9.56255814e-01  1.28195856e-01  1.88436434e-01 -2.42901670e-02\n   8.78196959e-02  1.60809159e-01 -8.13836136e-03  2.25782952e-01\n   3.83903673e+01]\n [ 2.36909466e+02 -3.33629219e+01  3.18556160e+01 -5.11872284e+02\n   2.31515659e+03  9.73171139e+01  1.41942171e+02 -1.26918819e+01\n   5.22928014e+01  4.44340177e+02  6.93777746e+00  3.83903673e+01\n   1.24265433e+05]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Το επόμενο βήμα είναι να υπολογίσετε τις <b><i>ιδιοτιμές</i></b> (<b><i>eigenvalues</i></b>) και τα <b><i>ιδιοδιανύσματα</i></b> (<b><i>eigenvectors</i></b>) του dataset.</p>","metadata":{"id":"rFxfxGxbiPsm"}},{"cell_type":"code","source":"values, vectors = eig(covariance)\nprint(\"The eigenvalues of the normalized data are the following: \")\nprint(values)","metadata":{"id":"nyqu1myLieP3","outputId":"814fe6ee-0258-4232-82f4-0a35b7e3d06a","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The eigenvalues of the normalized data are the following: \n[1.24313073e+05 1.93456026e+02 9.50168321e+00 1.32500169e+00\n 8.10595669e-01 4.51081713e-01 2.69157831e-01 1.60993380e-01\n 1.11100343e-01 6.13399000e-03 2.19782528e-02 3.20312852e-02\n 5.87421686e-02]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Στη συνέχεια, θα επιλέξετε τις πιο σημαντικές <b><i>ιδιοτιμές</i></b> και θα προσαρμόσετε αντίστοιχα τα <b><i>ιδιοδιανύσματα</i></b> του dataset.</p>","metadata":{"id":"51_NKEP2imoW"}},{"cell_type":"code","source":"new_values = values[0:3]\nprint(\"The most important eigenvalues are the following: \")\nprint(new_values)\nnew_vectors = vectors[0:3]\nprint(\"The most important eigenvectors are the following: \")\nprint(new_vectors)","metadata":{"id":"Dee3ENf1i4Lx","outputId":"7617d250-65bd-4ab0-a4d9-2dea8c9dfd03","trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The most important eigenvalues are the following: \n[1.24313073e+05 1.93456026e+02 9.50168321e+00]\nThe most important eigenvectors are the following: \n[[-1.90615566e-03 -4.75319302e-04 -4.70065618e-02  2.25255930e-01\n   9.82089183e-02 -2.20758056e-01 -8.52483156e-01 -3.65140031e-01\n  -1.65925760e-01 -5.62139324e-03 -2.07811823e-02  2.48378617e-02\n  -8.18675258e-04]\n [ 2.68379014e-04 -2.11118304e-03  3.21935310e-02  1.25115825e-01\n   9.16405147e-01 -3.07909300e-01  1.86306127e-01  7.26209733e-02\n  -1.81047372e-02 -1.16925805e-02  8.44468784e-02 -2.41209813e-02\n   2.31770029e-02]\n [-2.56407459e-04 -4.21644285e-03  5.19948847e-02  4.67286333e-02\n   2.01302236e-02 -3.45064791e-03 -1.57617050e-02  1.52544206e-01\n  -7.18891029e-02 -1.38330530e-01 -6.68720003e-02  6.75337422e-01\n  -6.97354965e-01]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Τώρα, θα εφαρμόσετε τα νέα <b><i>ιδιοδιανύσματα</i></b> στο παλιό dataset για να πάρετε το νέο, μειωμένο σε μέγεθος dataset.</p>","metadata":{"id":"6IVdHr-Ti-O5"}},{"cell_type":"code","source":"new_data = new_vectors.dot(data_normal.T) # (3,130)\nprint(new_data.shape)","metadata":{"id":"HtJouKmSjPbD","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(3, 130)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<p>Να αποθηκεύσετε το νέο dataset σε ένα αρχείο <i>csv</i>.</p>","metadata":{"id":"o5B93KQ3jTpf"}},{"cell_type":"code","source":"np.savetxt(\"foo.csv\", new_data.T, delimiter=\",\")","metadata":{"id":"W1uY6kFWjflf","trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<h4><b><i>Ερωτήσεις</i></b></h4>\n<ul>\n<li>1. Να μελετήσετε το παραπάνω πρόγραμμα και να περιγράψετε, σύντομα, τα βήματα που ακολουθεί ο αλγόριθμος <b><i>PCA</i></b>, υλοποιημένος με τη μέθοδο <b><i>covariance</i></b>. Να συμπεριλάβετε και τις μαθηματικές πράξεις.</li>\n<li>2. Να εφαρμόσετε τον αλγόριθμο <b><i>PCA</i></b> πάνω στα δεδομένα του αρχείου <i><a href=\"https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab2/demo3b.csv\">demo3b.csv</a></i>.  Στη συνέχεια, να καταγράψετε τον <i>πίνακα συνδιακύμανσης</i> του dataset και τις ιδιοτιμές του πίνακα αυτού. Τι υποδηλώνουν οι θετικές και τι οι αρνητικές τιμές του <i>πίνακα συνδιακύμανσης</i>; Να διατάξετε τις <i>ιδιοτιμές</i> σε φθίνουσα σειρά. Τι παρατηρείτε για τις τρεις πρώτες σε σχέση με τις υπόλοιπες; Πόσες <i>κύριες συνιστώσες</i> επιλέγει να διατηρήσει ο αλγόριθμος;</li>\n</ul>","metadata":{"id":"8u085fpWjnPD"}},{"cell_type":"markdown","source":"### Aπαντήσεις \n\n#### 1. Βήματα PCA:\n\nA. Οι διαστάσεις του dataset είναι (130, 13). Aφού πάρουμε το transpose του πίνακα (13,130) υπολογίζουμε το mean value ανα στήλη (13 Mean Values) και τα αφαιρούμε από κάθε τιμή σε κάθε γραμμή (από την κάθε διάσταση του πίνακα). \n$$ mean = \\frac {\\sum_{i=1}^{13} data(i,:)} { 13} $$\n$$ data_n = data -mean $$ \n\n        \n        Μιας άλλης μορφης normalization ειναι ο z-Normalization: \n$$ data_Z  = \\frac {data - mean} { std } $$\n\nB. Υπολογίζουμε το Covarianve Matrix (13,13) αφού ο πίνακας έχει 13 διαστάσεις.\n\n$$ cov(data_1...data_{13}) = \\frac {(\\sum_{i=1}^{n=13} data_i - mean)} {n-1}  $$\n\nC. Υπολογίζουμε τα eigenvectors(13,13) & eigenvalues(13) του covariance matrix.\n\n    \n      v: eigenvector λ: eigenvalue I: identity matrix  A: Covariance matrix\n$$ A  v = \\lambda v => A  v = I \\lambda v => A  v - I \\lambda v = 0 $$ \n$$ | A - \\lambda I | = 0 $$\n\n        Βρίσκουμε τα eigenvalues λύνοντας το παραπάνω σύστημα.\n\nD. Aπό τα eigenvalues και eigenvectors που υπολογισαμε παπραπάνω κρατάμε τα 3 eigenvalues (μεγαλύτερα) και τα αντίστοιχα eigenvectors.\n\n$$ sort(eigenvalues) => Feature Vector = (eig_1, eig_2, eig_3, ...)$$\n\nE. Πολλαπλασιάζοντας τα αρχικά μας 'κανονικοποιημένα' δεδομένα  (130,13) με το διάνυσμα των παραπάνω eigenvectors(13,3) παίρνουμε τα τελικά δεδομένα (130,3) του PCA αξιοποιώντας τα 3 πιο principal components.\n\n$$ Final Data = RowFeatureVector   *  RowDataAdjust $$ ","metadata":{}},{"cell_type":"markdown","source":"#### 2. PCA \nTρέξαμε τον αλγόριθμο PCA στο **demo3b.csv** ακολουθώντας τα βήματα που αναφέρθηκαν παραπάνω \n\nΟι θετικές τιμές στον Covarianve matrix ανάμεσα σε 2 τιμές, σημάινουν ταυτόχρονη αύξηση των εν λογω μεταβλητών, ενώ αρνήτικη τιμή σημαίνει αύξηση της μιας μεταβλητής όταν η άλλη μειώνεται. ","metadata":{}},{"cell_type":"code","source":"print('Here is the covariance matrix (13,13):')\ncovariance\n","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Here is the covariance matrix (13,13):\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[ 7.89911157e-01,  1.64350328e-02,  5.03965534e-02,\n        -1.39557561e+00,  4.51870543e+00,  2.35123047e-01,\n         3.56524806e-01, -2.92133512e-02,  9.15320632e-02,\n         1.07205560e+00,  3.92238044e-03,  1.22678110e-01,\n         2.36909466e+02],\n       [ 1.64350328e-02,  7.74344335e-01,  3.18039952e-02,\n         4.24165474e-01, -2.21434109e-01,  1.44160704e-02,\n         3.75581395e-02,  5.34385808e-03,  6.47891175e-02,\n        -1.54797734e-01, -6.06599761e-02,  6.96007454e-02,\n        -3.33629219e+01],\n       [ 5.03965534e-02,  3.18039952e-02,  8.83052117e-02,\n         3.73006798e-01,  1.40136434e+00,  4.12452594e-02,\n         8.26573643e-02,  5.81539654e-03,  1.24709839e-02,\n         1.22397007e-01,  2.06346094e-03,  3.02871079e-02,\n         3.18556160e+01],\n       [-1.39557561e+00,  4.24165474e-01,  3.73006798e-01,\n         1.15631181e+01, -6.46193798e+00, -4.24464222e-01,\n        -4.52046512e-01,  1.24282469e-01, -1.77910614e-01,\n        -2.39565069e+00, -2.05387955e-02, -1.02069171e-03,\n        -5.11872284e+02],\n       [ 4.51870543e+00, -2.21434109e-01,  1.40136434e+00,\n        -6.46193798e+00,  2.36586822e+02,  2.55096899e+00,\n         2.89720930e+00, -3.57263566e-01,  2.30737209e+00,\n         8.63970543e+00,  1.85996899e-01,  9.56255814e-01,\n         2.31515659e+03],\n       [ 2.35123047e-01,  1.44160704e-02,  4.12452594e-02,\n        -4.24464222e-01,  2.55096899e+00,  2.97453936e-01,\n         3.40611628e-01, -2.64587657e-02,  1.30694305e-01,\n         5.23527370e-01, -7.63816339e-04,  1.28195856e-01,\n         9.73171139e+01],\n       [ 3.56524806e-01,  3.75581395e-02,  8.26573643e-02,\n        -4.52046512e-01,  2.89720930e+00,  3.40611628e-01,\n         5.44297674e-01, -2.88767442e-02,  2.16111628e-01,\n         8.48084496e-01, -8.24186047e-04,  1.88436434e-01,\n         1.41942171e+02],\n       [-2.92133512e-02,  5.34385808e-03,  5.81539654e-03,\n         1.24282469e-01, -3.57263566e-01, -2.64587657e-02,\n        -2.88767442e-02,  1.18999463e-02, -1.98501670e-02,\n        -4.97212642e-02,  9.44753727e-04, -2.42901670e-02,\n        -1.26918819e+01],\n       [ 9.15320632e-02,  6.47891175e-02,  1.24709839e-02,\n        -1.77910614e-01,  2.30737209e+00,  1.30694305e-01,\n         2.16111628e-01, -1.98501670e-02,  2.91137680e-01,\n         2.39266834e-01, -9.77493143e-04,  8.78196959e-02,\n         5.22928014e+01],\n       [ 1.07205560e+00, -1.54797734e-01,  1.22397007e-01,\n        -2.39565069e+00,  8.63970543e+00,  5.23527370e-01,\n         8.48084496e-01, -4.97212642e-02,  2.39266834e-01,\n         2.64316778e+00,  2.68330829e-03,  1.60809159e-01,\n         4.44340177e+02],\n       [ 3.92238044e-03, -6.06599761e-02,  2.06346094e-03,\n        -2.05387955e-02,  1.85996899e-01, -7.63816339e-04,\n        -8.24186047e-04,  9.44753727e-04, -9.77493143e-04,\n         2.68330829e-03,  2.84562519e-02, -8.13836136e-03,\n         6.93777746e+00],\n       [ 1.22678110e-01,  6.96007454e-02,  3.02871079e-02,\n        -1.02069171e-03,  9.56255814e-01,  1.28195856e-01,\n         1.88436434e-01, -2.42901670e-02,  8.78196959e-02,\n         1.60809159e-01, -8.13836136e-03,  2.25782952e-01,\n         3.83903673e+01],\n       [ 2.36909466e+02, -3.33629219e+01,  3.18556160e+01,\n        -5.11872284e+02,  2.31515659e+03,  9.73171139e+01,\n         1.41942171e+02, -1.26918819e+01,  5.22928014e+01,\n         4.44340177e+02,  6.93777746e+00,  3.83903673e+01,\n         1.24265433e+05]])"},"metadata":{}}]},{"cell_type":"markdown","source":"- Διάταξη των ιδιοτιμών σε αύξουσα σειρά ","metadata":{}},{"cell_type":"code","source":"print(\"The eigenvalues of the normalized data are the following: \")\nsorted_eigenvalues = np.sort(values)\nprint(sorted_eigenvalues[::-1])\n\nplt.figure(figsize=(20,10))\nplt.plot(sorted_eigenvalues[::-1], '*')\nplt.title('Eigenvalues in Decreasing Order ')","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The eigenvalues of the normalized data are the following: \n[1.24313073e+05 1.93456026e+02 9.50168321e+00 1.32500169e+00\n 8.10595669e-01 4.51081713e-01 2.69157831e-01 1.60993380e-01\n 1.11100343e-01 5.87421686e-02 3.20312852e-02 2.19782528e-02\n 6.13399000e-03]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9114a2c378a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_eigenvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_eigenvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eigenvalues in Decreasing Order '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Απο τις sorted eigenvalues παρατηρούμε ότι η πρώτη κύρια τιμή είναι αρκετές τάξεις μεγέθους μεγαλύτερη απο τις υπόλοιπες τιμές. Μετά τα 4 πρώτα eigenvalues παρατηρούμε ταχεία συρίκνωση των υπόλοιπων τιμων.","metadata":{}},{"cell_type":"markdown","source":"<h3><b><i>Logistic Regression</i></b></h3>\n\n<p>Αρχικά, θα φορτώσετε τις απαραίτητες βιβλιοθήκες.</p>","metadata":{"id":"DBRxVL9BmqGw"}},{"cell_type":"code","source":"#!pip install pandas\n#!pip install numpy\n#!pip install matplotlib\n#!pip install sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression","metadata":{"id":"MiGjiOEQnpwn","outputId":"816c0f5e-49cd-45db-beb2-1725ed25fbd5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Στη συνέχεια, θα φορτώσετε τα datasets που θα χρησιμοποιηθούν για την εκπαίδευση του μοντέλου <b><i>logistic regression</i></b>. Στην πρώτη περίπτωση, θα εκπαιδεύσετε το μοντέλο, χρησιμοποιώντας το αρχείο <i><a href=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\">demo3a.csv</a></i>. Στη δεύτερη περίπτωση, θα φορτώσετε το αρχείο <i>foo.csv</i> που πήρατε ως έξοδο από το προηγούμενο τμήμα κώδικα της άσκησης (<i>Ανάλυση σε Κύριες Συνιστώσες</i>), προσθέτοντας στην πρώτη στήλη του αρχείου τα labels που υπάρχουν στην πρώτη στήλη του αρχείου <a href=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\"><i>demo3a.csv</i></a>.","metadata":{"id":"JnNo_v4Jn3gn"}},{"cell_type":"code","source":"# demo3a.csv\nimport pandas as pd \ndf_01 = pd.read_csv(\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\", header = None)","metadata":{"id":"7cJDDmjxpSTT","trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Separate the input features from the target variable\n\nx = df_01.iloc[:,1:13].values\ny = df_01.iloc[:,0].values","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_01","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     0      1     2     3     4    5     6     7     8     9     10    11  \\\n0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04   \n1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05   \n2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03   \n3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86   \n4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04   \n..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n125   2  12.07  2.16  2.17  21.0   85  2.60  2.65  0.37  1.35  2.76  0.86   \n126   2  12.43  1.53  2.29  21.5   86  2.74  3.15  0.39  1.77  3.94  0.69   \n127   2  11.79  2.13  2.78  28.5   92  2.13  2.24  0.58  1.76  3.00  0.97   \n128   2  12.37  1.63  2.30  24.5   88  2.22  2.45  0.40  1.90  2.12  0.89   \n129   2  12.04  4.30  2.38  22.0   80  2.10  1.75  0.42  1.35  2.60  0.79   \n\n       12    13  \n0    3.92  1065  \n1    3.40  1050  \n2    3.17  1185  \n3    3.45  1480  \n4    2.93   735  \n..    ...   ...  \n125  3.28   378  \n126  2.84   352  \n127  2.44   466  \n128  2.78   342  \n129  2.57   580  \n\n[130 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>2</td>\n      <td>12.07</td>\n      <td>2.16</td>\n      <td>2.17</td>\n      <td>21.0</td>\n      <td>85</td>\n      <td>2.60</td>\n      <td>2.65</td>\n      <td>0.37</td>\n      <td>1.35</td>\n      <td>2.76</td>\n      <td>0.86</td>\n      <td>3.28</td>\n      <td>378</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>2</td>\n      <td>12.43</td>\n      <td>1.53</td>\n      <td>2.29</td>\n      <td>21.5</td>\n      <td>86</td>\n      <td>2.74</td>\n      <td>3.15</td>\n      <td>0.39</td>\n      <td>1.77</td>\n      <td>3.94</td>\n      <td>0.69</td>\n      <td>2.84</td>\n      <td>352</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>2</td>\n      <td>11.79</td>\n      <td>2.13</td>\n      <td>2.78</td>\n      <td>28.5</td>\n      <td>92</td>\n      <td>2.13</td>\n      <td>2.24</td>\n      <td>0.58</td>\n      <td>1.76</td>\n      <td>3.00</td>\n      <td>0.97</td>\n      <td>2.44</td>\n      <td>466</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>2</td>\n      <td>12.37</td>\n      <td>1.63</td>\n      <td>2.30</td>\n      <td>24.5</td>\n      <td>88</td>\n      <td>2.22</td>\n      <td>2.45</td>\n      <td>0.40</td>\n      <td>1.90</td>\n      <td>2.12</td>\n      <td>0.89</td>\n      <td>2.78</td>\n      <td>342</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>2</td>\n      <td>12.04</td>\n      <td>4.30</td>\n      <td>2.38</td>\n      <td>22.0</td>\n      <td>80</td>\n      <td>2.10</td>\n      <td>1.75</td>\n      <td>0.42</td>\n      <td>1.35</td>\n      <td>2.60</td>\n      <td>0.79</td>\n      <td>2.57</td>\n      <td>580</td>\n    </tr>\n  </tbody>\n</table>\n<p>130 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<p>Έπειτα, θα χωρίσετε το dataset σε <i>training</i> και <i>test set</i>.</p>","metadata":{"id":"nyQn9Hjapdjj"}},{"cell_type":"code","source":"df_02 = pd.read_csv(\"foo.csv\", names=['eigenvalue_01', 'eigenvalue_02', 'eigenvalue_03'] )\ndf_02['Target'] = df_01[0]","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Separate the input features from the target variable\n\n# Keep 3 Principal Components \nx = df_02.iloc[:,0:3].values\ny = df_02.iloc[:,3].values","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Keep 2 Principal Components \nx = df_02.iloc[:,0:2].values\ny = df_02.iloc[:,3].values","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Keep 1 Principal Components \nx = df_02.iloc[:,0:1].values\ny = df_02.iloc[:,3].values","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train and test set\nfrom sklearn.model_selection import train_test_split\nXtrain,Xtest,Ytrain,Ytest = train_test_split(x,y, test_size = 0.2)","metadata":{"id":"NuWbSbVwpWUe","trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<p>Έπειτα, θα εκπαιδεύσετε το μοντέλο <b><i>Logistic Regression</i></b></p>","metadata":{"id":"HFvPctcepyai"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression(max_iter = 1000)\nclassifier.fit(Xtrain,Ytrain)","metadata":{"id":"cDquqsFOpuUl","outputId":"299caf32-19da-4474-cac3-471a90969ea3","trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(max_iter=1000)"},"metadata":{}}]},{"cell_type":"markdown","source":"<p>Τέλος, θα λάβετε τις προβλέψεις του μοντέλου που εκπαιδεύσατε πάνω στο test set και θα υπολογίσετε τα συνολικά λάθη.</p>","metadata":{"id":"ZGx2iFUlqC4w"}},{"cell_type":"code","source":"# Get the predictions on the test set\nprediction = classifier.predict(Xtest)\n\n# Calculate the total number of errors on the test set\nerrors = 0\nfor index in range(0,len(prediction) - 1):\n    if prediction[index] != Ytest[index]:\n        errors += 1\n\nprint(\"Total errors on the test dataset\")\nprint(errors)","metadata":{"id":"opOmr-vCqOKL","outputId":"4212dd79-96cc-417c-d703-81228dbd6623","trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Total errors on the test dataset\n10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h4><b><i>Ερώτηση</i></b></h4>\n<p>Να εκτελέσετε παραπάνω τμήματα κώδικα με είσοδο τα αρχεία (a) <i><a href=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\">demo3a.csv</a></i> και (b) <i>foo.csv</i>. Τι παρατηρείτε για την ακρίβεια του μοντέλου στις δύο περιπτώσεις; Δοκιμάστε και για την περίπτωση που κρατάμε (α) 1 και (β) 2 <i>κύριες συνιστώσες</i>. Τι παρατηρείτε;</p>","metadata":{"id":"xRVRieelqVsA"}},{"cell_type":"markdown","source":"#### **Απάντηση**\n\nDataset:  **demo3a.csv**  Aριθμός σφαλμάτων: **2**. \n\nDataset:  **foo.csv**     Principal Components: 3    Aριθμός σφαλμάτων: **1**. \n\nDataset:  **foo.csv**     Principal Components: 2    Aριθμός σφαλμάτων: **2**. \n\nDataset:  **foo.csv**     Principal Components: 1    Aριθμός σφαλμάτων: **9**. \n\n> Παρατηρούμε ότι η καλύτερη απόδοση της PCA ανάλυσης  είναι για **n_components=3**. Ενώ στην περίπτωση με **n_components=1** η απόδοση του μοντέλου είναι > σημαντικά χειρότερη από ότι να χρησιμοποιούσαμε το αρχικό dataset χωρίς καμία διαδικασία decomposition. O λόγος είναι ότι η διακύμανση των δεδομένων δέν μπορεί να επεξηγηθεί μόνο με ένα principal component.\n\nΓια την επαλήθευση του αριθμού των απαιτούμενων principal components που επεξηγούν το variance του παρόντος dataset, χρησιμοποιούμε ένα **screen plot** αντίστοιχο της **elbow method** από το Κ-means. Ωστόσο ενώ στο παρακάτω διάγραμμα 2 principal components φαίνεται να είναι επαρκή για την επεξήγηση του variance, ωστόσο η απόδοση στο test set είναι καλύτερη για 3 principal components.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler # Z-Normalization\n# Read the DataSet\ndf_01 = pd.read_csv(\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\", header = None)\n\nscaler = StandardScaler()\nscaled_df = pd.DataFrame(scaler.fit_transform(df_01))\nscaled_df.head()","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         0         1         2         3         4         5         6   \\\n0 -1.096991  1.452455 -0.294414  0.302478 -0.940375  1.768686  0.510421   \n1 -1.096991  0.289066 -0.214558 -0.677197 -2.239324  0.006527  0.234327   \n2 -1.096991  0.243886  0.447106  1.113242 -0.054728  0.071792  0.510421   \n3 -1.096991  1.610586 -0.020622  0.538951 -0.586117  0.854974  2.443085   \n4 -1.096991  0.334246  0.709490  1.788880  0.653789  1.181300  0.510421   \n\n         7         8         9         10        11        12        13  \n0  0.775592 -0.462247  1.000229  0.892384 -0.112428  2.040025  0.782868  \n1  0.367386 -0.646296 -0.878867  0.114374 -0.052918  0.941437  0.740152  \n2  1.020516 -0.278198  1.967684  0.917082 -0.171937  0.455524  1.124598  \n3  1.360688 -0.830346  0.795575  2.226115 -1.183603  1.047071  1.964684  \n4  0.272138  0.550024  0.125798  0.077326 -0.112428 -0.051517 -0.156889  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.096991</td>\n      <td>1.452455</td>\n      <td>-0.294414</td>\n      <td>0.302478</td>\n      <td>-0.940375</td>\n      <td>1.768686</td>\n      <td>0.510421</td>\n      <td>0.775592</td>\n      <td>-0.462247</td>\n      <td>1.000229</td>\n      <td>0.892384</td>\n      <td>-0.112428</td>\n      <td>2.040025</td>\n      <td>0.782868</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.096991</td>\n      <td>0.289066</td>\n      <td>-0.214558</td>\n      <td>-0.677197</td>\n      <td>-2.239324</td>\n      <td>0.006527</td>\n      <td>0.234327</td>\n      <td>0.367386</td>\n      <td>-0.646296</td>\n      <td>-0.878867</td>\n      <td>0.114374</td>\n      <td>-0.052918</td>\n      <td>0.941437</td>\n      <td>0.740152</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.096991</td>\n      <td>0.243886</td>\n      <td>0.447106</td>\n      <td>1.113242</td>\n      <td>-0.054728</td>\n      <td>0.071792</td>\n      <td>0.510421</td>\n      <td>1.020516</td>\n      <td>-0.278198</td>\n      <td>1.967684</td>\n      <td>0.917082</td>\n      <td>-0.171937</td>\n      <td>0.455524</td>\n      <td>1.124598</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.096991</td>\n      <td>1.610586</td>\n      <td>-0.020622</td>\n      <td>0.538951</td>\n      <td>-0.586117</td>\n      <td>0.854974</td>\n      <td>2.443085</td>\n      <td>1.360688</td>\n      <td>-0.830346</td>\n      <td>0.795575</td>\n      <td>2.226115</td>\n      <td>-1.183603</td>\n      <td>1.047071</td>\n      <td>1.964684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.096991</td>\n      <td>0.334246</td>\n      <td>0.709490</td>\n      <td>1.788880</td>\n      <td>0.653789</td>\n      <td>1.181300</td>\n      <td>0.510421</td>\n      <td>0.272138</td>\n      <td>0.550024</td>\n      <td>0.125798</td>\n      <td>0.077326</td>\n      <td>-0.112428</td>\n      <td>-0.051517</td>\n      <td>-0.156889</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.decomposition import PCA \npca = PCA(n_components=8)\ndata_PCA = pca.fit_transform(scaled_df)\ndf_PCA = pd.DataFrame(data_PCA)\ndf_PCA","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"            0         1         2         3         4         5         6  \\\n0    3.313518  0.169972 -0.235209 -0.233681  0.816913 -0.839228  0.196497   \n1    1.525604 -1.413705 -1.243934  0.624322 -0.494663 -0.923863  0.332499   \n2    2.521503  1.053352  0.294206 -0.131670  0.240177  0.939082  0.300815   \n3    4.716520  0.449422 -0.255112  0.563082 -0.215629  0.209059 -1.199817   \n4    1.037272  1.265576  1.774692  0.455036  0.652832 -0.206063  0.080311   \n..        ...       ...       ...       ...       ...       ...       ...   \n125 -1.804094  1.343587 -0.962781  0.148253 -1.059861 -0.186065 -0.721303   \n126 -1.150152  1.422130 -0.661820  0.355375 -0.949928  0.930870 -2.094807   \n127 -2.942398  2.084394  2.589530  0.034890 -0.239658  1.013783 -0.704468   \n128 -2.371724  1.349047 -0.014339 -0.317954 -0.328490  0.406828 -0.832766   \n129 -2.842556  1.919982 -0.184156  2.357618 -0.155725  0.373290  0.448293   \n\n            7  \n0   -0.303325  \n1    0.522603  \n2   -1.100457  \n3    0.192571  \n4    0.356968  \n..        ...  \n125  0.464987  \n126 -0.068552  \n127 -0.624722  \n128 -0.693503  \n129  0.188522  \n\n[130 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.313518</td>\n      <td>0.169972</td>\n      <td>-0.235209</td>\n      <td>-0.233681</td>\n      <td>0.816913</td>\n      <td>-0.839228</td>\n      <td>0.196497</td>\n      <td>-0.303325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.525604</td>\n      <td>-1.413705</td>\n      <td>-1.243934</td>\n      <td>0.624322</td>\n      <td>-0.494663</td>\n      <td>-0.923863</td>\n      <td>0.332499</td>\n      <td>0.522603</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.521503</td>\n      <td>1.053352</td>\n      <td>0.294206</td>\n      <td>-0.131670</td>\n      <td>0.240177</td>\n      <td>0.939082</td>\n      <td>0.300815</td>\n      <td>-1.100457</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.716520</td>\n      <td>0.449422</td>\n      <td>-0.255112</td>\n      <td>0.563082</td>\n      <td>-0.215629</td>\n      <td>0.209059</td>\n      <td>-1.199817</td>\n      <td>0.192571</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.037272</td>\n      <td>1.265576</td>\n      <td>1.774692</td>\n      <td>0.455036</td>\n      <td>0.652832</td>\n      <td>-0.206063</td>\n      <td>0.080311</td>\n      <td>0.356968</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>-1.804094</td>\n      <td>1.343587</td>\n      <td>-0.962781</td>\n      <td>0.148253</td>\n      <td>-1.059861</td>\n      <td>-0.186065</td>\n      <td>-0.721303</td>\n      <td>0.464987</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>-1.150152</td>\n      <td>1.422130</td>\n      <td>-0.661820</td>\n      <td>0.355375</td>\n      <td>-0.949928</td>\n      <td>0.930870</td>\n      <td>-2.094807</td>\n      <td>-0.068552</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>-2.942398</td>\n      <td>2.084394</td>\n      <td>2.589530</td>\n      <td>0.034890</td>\n      <td>-0.239658</td>\n      <td>1.013783</td>\n      <td>-0.704468</td>\n      <td>-0.624722</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>-2.371724</td>\n      <td>1.349047</td>\n      <td>-0.014339</td>\n      <td>-0.317954</td>\n      <td>-0.328490</td>\n      <td>0.406828</td>\n      <td>-0.832766</td>\n      <td>-0.693503</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>-2.842556</td>\n      <td>1.919982</td>\n      <td>-0.184156</td>\n      <td>2.357618</td>\n      <td>-0.155725</td>\n      <td>0.373290</td>\n      <td>0.448293</td>\n      <td>0.188522</td>\n    </tr>\n  </tbody>\n</table>\n<p>130 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nPCA_values = np.arange(pca.n_components_)+1\nplt.plot(PCA_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\nplt.title('Screen Plot')","metadata":{"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Screen Plot')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"<h3><b><i>Συμπληρωματικές Ερωτήσεις</i></b></h3>\n<ul>\n<li>1. Ποια είναι η χρησιμότητα του αλγορίθμου <b><i>PCA</i></b> ως προς τη δυνατότητα <i>οπτικοποίησης</i> (<i>visualization</i>) των δεδομένων του dataset;</li>\n<li>2. Ποια είναι η χρησιμότητα του αλγορίθμου <b><i>PCA</i></b> ως προς την ταχύτητα εκπαίδευσης του μοντέλου logistic regression; Να βασίσετε την απάντησή σας στα δύο παρακάτω τμήματα κώδικα.</li>\n</ul>","metadata":{"id":"gqPhHDExffku"}},{"cell_type":"markdown","source":"#### 1. Xρησιμότητα του PCA\n> H χρησιμότητα του αλγορίθμου PCA είναι η μείωση των διαστάσεων των δεδομένων, η αναπαράστασή της δηλαδή με την χρήση λιγότερων συνιστώσεων. Με την οπτικοποίηση των δεδομένων χρησιμοποιώντας PCA κατανοούμε καλύτερα την συσχέτιση μεταξύ των χαρακτηριστικών του dataset. ","metadata":{}},{"cell_type":"markdown","source":"#### 2. PCA και ταχύτητα εκπαίδευσης \n> Με την χρήση κάποιων (μεγαλύτερων) principal components αντί του συνολικού dataset η ταχύτητα εκπαίδευσης του μοντέλου αναμένεται να συρικνωθεί σημαντικά. Παρόλα αυτά στο εν λόγο dataset λόγω του μικρού του μεγέθους δεν είναι ιδιαίτερα αισθητή η μείωση του χρόνου εκπαίδευσης κατά την εκτέλεση των εντολών ωστόσο με τον πιο κάτω κώδικα παρατηρούμε μείωση στον χρόνο εκπαίδευσης. \n\n> **Training_Timings**","metadata":{}},{"cell_type":"code","source":"import time \n\n# demo3a.csv\ndf_01 = pd.read_csv(\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab2/demo3a.csv\", header = None)\n\n# Separate the input features from the target variable\n\nx = df_01.iloc[:,1:13].values\ny = df_01.iloc[:,0].values\n\nXtrain,Xtest,Ytrain,Ytest = train_test_split(x,y, test_size = 0.2)\n\ntime_table=[]\n\nstart_time = time.time()\nclassifier = LogisticRegression(max_iter = 1000)\nclassifier.fit(Xtrain,Ytrain)\ntime_table.append(time.time() - start_time)\n\n######################################################################\ndf_02 = pd.read_csv(\"foo.csv\", names=['eigenvalue_01', 'eigenvalue_02', 'eigenvalue_03'] )\ndf_02['Target'] = df_01[0]\n\n\n\n# Keep 3 Principal Components \n\nx = df_02.iloc[:,0:3].values\ny = df_02.iloc[:,3].values\n\nXtrain,Xtest,Ytrain,Ytest = train_test_split(x,y, test_size = 0.2)\n\nstart_time = time.time()\nclassifier = LogisticRegression(max_iter = 1000)\nclassifier.fit(Xtrain,Ytrain)\ntime_table.append(time.time() - start_time)\n\n######################################################################\nx = df_02.iloc[:,0:2].values\ny = df_02.iloc[:,3].values\n\nXtrain,Xtest,Ytrain,Ytest = train_test_split(x,y, test_size = 0.2)\n\nstart_time = time.time()\nclassifier = LogisticRegression(max_iter = 1000)\nclassifier.fit(Xtrain,Ytrain)\ntime_table.append(time.time() - start_time)\n######################################################################\nx = df_02.iloc[:,0:2].values\ny = df_02.iloc[:,3].values\n\nXtrain,Xtest,Ytrain,Ytest = train_test_split(x,y, test_size = 0.2)\n\nstart_time = time.time()\nclassifier = LogisticRegression(max_iter = 1000)\nclassifier.fit(Xtrain,Ytrain)\ntime_table.append(time.time() - start_time)\n\ntime_table = np.array(time_table)\nprint(time_table)\n","metadata":{"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[0.07343578 0.00759649 0.00642586 0.00560427]\n","output_type":"stream"}]},{"cell_type":"code","source":"d = {'All_Data': [time_table[0]], 'PCA_03': [time_table[1]],'PCA_02': [time_table[2]],'PCA_01': [time_table[3]]}\nTraining_Timings  = pd.DataFrame(data=d)\n\nprint(' Training Timings in seconds:\\n',Training_Timings)\n","metadata":{"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":" Training Timings in seconds:\n    All_Data    PCA_03    PCA_02    PCA_01\n0  0.073436  0.007596  0.006426  0.005604\n","output_type":"stream"}]}]}